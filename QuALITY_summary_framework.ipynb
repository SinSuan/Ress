{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_process import get_data\n",
    "path=\"./dataset/Final_Quality.json\"\n",
    "data=get_data(path)\n",
    "print(data[0].keys())\n",
    "print(f\"資料數:{len(data)}\")\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Choose Model Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選擇使用模型 Breeze Taide ChatGPT Llama3 Mistral\n",
    "model_name=\"Breeze\" #Breeze Taide ChatGPT Llama3 Mistral\n",
    "from utils.api_devlop import get_api\n",
    "get_llm_reply=get_api(model_name)\n",
    "print(get_llm_reply(\"請用中文自我介紹，你是誰訓練的模型\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\").to(device)\n",
    "device = model.device\n",
    "\n",
    "def pick_up(result):\n",
    "    if \"1\" in result:\n",
    "        return 1\n",
    "    elif \"2\" in result:\n",
    "        return 2\n",
    "    elif \"3\" in result:\n",
    "        return 3\n",
    "    elif \"4\" in result:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "def split_with_overlap_english(text, chunk_size=3000, overlap=1):\n",
    "    chunks=[]\n",
    "    sentences=text.split(\".\")\n",
    "    temp_chunk=[]\n",
    "\n",
    "    pre_overlap_num=0\n",
    "    pre_overlap=[]\n",
    "    af_overlap=[]\n",
    "    word_count=\"\"\n",
    "    for sentence_idx in range(len(sentences)):\n",
    "        word_count+=sentences[sentence_idx]+\" \"\n",
    "        temp_chunk.append(sentences[sentence_idx])\n",
    "\n",
    "        if len(word_count.split(\" \"))>chunk_size:\n",
    "            if chunks!=[]:\n",
    "                pre_overlap_num=-overlap\n",
    "                pre_overlap=chunks[-1].split(\".\")[pre_overlap_num*2:pre_overlap_num]\n",
    "            if sentence_idx<len(sentences)-1:\n",
    "                af_overlap_num=min(len(sentences)-1,sentence_idx+1+overlap)\n",
    "                af_overlap=sentences[sentence_idx+1:af_overlap_num]\n",
    "            chunks.append(\". \".join(pre_overlap)+\". \"+\". \".join(temp_chunk)+\". \"+\". \".join(af_overlap))\n",
    "            temp_chunk.clear()\n",
    "            word_count=\"\"\n",
    "    if chunks!=[]:\n",
    "        pre_overlap_num=-overlap\n",
    "        pre_overlap=chunks[-1].split(\".\")[pre_overlap_num*2:pre_overlap_num]\n",
    "    chunks.append(\". \".join(pre_overlap)+\". \"+\". \".join(temp_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def find_smaller_than_neighbors(arr):\n",
    "    result = []\n",
    "    for i in range(1, len(arr) - 1):\n",
    "        if arr[i] < arr[i - 1] and arr[i] < arr[i + 1]:\n",
    "            result.append(i)\n",
    "    return result\n",
    "\n",
    "def Semantic_Sentence_Split(text, chunk_size=3000, overlap=10):\n",
    "    # 將句子以句號分割\n",
    "    sentences = text.split(\".\")\n",
    "    # 將句子進行encode\n",
    "    embedding_sentences = model.encode(sentences)\n",
    "    # 計算句子倆倆相似度\n",
    "    dot_result = []\n",
    "    for i in range(len(embedding_sentences) - 1):\n",
    "        similarity_of_adjacent = util.dot_score(embedding_sentences[i], embedding_sentences[i + 1])\n",
    "        dot_result.append(similarity_of_adjacent.item())\n",
    "    smaller_than_neighbors = find_smaller_than_neighbors(dot_result)\n",
    "    start_num = 0\n",
    "    passages = []\n",
    "    for num in smaller_than_neighbors:\n",
    "        end_num = num + 1  # 确定当前段落的结束位置\n",
    "        if len(\". \".join(sentences[start_num:end_num]).split(\" \")) > chunk_size:\n",
    "            temp_passage = \". \".join(sentences[start_num:end_num])  # 创建当前段落\n",
    "            passages.append(temp_passage)\n",
    "            start_num = max(start_num + 1, num + 1 -overlap)  # 更新下一段的起始位置，添加重叠\n",
    "    temp_passage = \". \".join(sentences[start_num:])  # 添加最后一段\n",
    "    passages.append(temp_passage)\n",
    "    return passages\n",
    "\n",
    "\n",
    "def os_ap_sss_answer(summary_prompt,input_content,input_question,truncate_size=3000,overlap=10):\n",
    "    \"\"\"\n",
    "    url_num 多個api要用哪張顯卡 multithread 用的\n",
    "    input_content 輸入文章\n",
    "    input_question 輸入問題\n",
    "    truncate_size 最常吃的input\n",
    "    overlap 切完文章後要 overlap 多少句\n",
    "    \"\"\"\n",
    "    os_prompt=summary_prompt\n",
    "    \n",
    "    # 切分摘要完要輸入給llm的內容\n",
    "    new_content=input_content\n",
    "    while len(new_content.split(\" \"))>truncate_size:\n",
    "        # 將內容先切成好幾個chunk\n",
    "        content_chuncks=Semantic_Sentence_Split(new_content,truncate_size,overlap)\n",
    "        # 這一輪的新內容\n",
    "        new_content=\"\"\n",
    "        for chunk in content_chuncks:\n",
    "\n",
    "            # 請llm幫我們把重要資訊留下\n",
    "            input_for_reader=f\"\"\"Article excerpt:\n",
    "            {chunk}\n",
    "            \n",
    "            The above is the article excerpt related to my question.\n",
    "            Below is the question I want to ask.\n",
    "            Please select the text content that can answer this question.\n",
    "            {os_prompt}\n",
    "            \n",
    "            Question:\n",
    "            {input_question}\"\"\"\n",
    "            # 將llm認為有關係的地方留下來\n",
    "            chunk_summary=get_llm_reply(input_for_reader)\n",
    "            # if chunk_summary==None:\n",
    "            #     continue\n",
    "            new_content+=chunk_summary+\" \"\n",
    "        # 防錯(如果LLM api無回傳 直接比照truncate)\n",
    "        if new_content==\"\":\n",
    "            new_content=\" \".join(input_content.split(\" \")[:3000])\n",
    "            break\n",
    "    # 找完有用的內容後，進行問答\n",
    "    input_to_llm=f\"\"\"There will be an article question and four options. \n",
    "    Please choose the option that answers the question based on the article.\n",
    "\n",
    "    article:\n",
    "    {new_content}\n",
    "\n",
    "    question:\n",
    "    {input_question}\n",
    "    \n",
    "    Your answer must be the number of one of the options,meaning it should be either option1, option2, option3, or option4. \n",
    "    The format for the answer should be as follows: Answer__optionX.\"\"\"\n",
    "    answer_from_llm=get_llm_reply(input_to_llm)\n",
    "    return answer_from_llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 練蠱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422b39d60660427b977219178d82a3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f1064c990f45a99f6c1dbabce1a9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd123a2055f8445291978cc0e0be2f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from api.Breeze_API_t import get_breeze_t\n",
    "\n",
    "def get_score(chunck_size,overlap,prompt=\"\",test_data=[]):\n",
    "    # 總分數\n",
    "    scores=0 \n",
    "    # 資料計數器\n",
    "    num=0 \n",
    "    for each_data in tqdm(test_data):\n",
    "        try:\n",
    "            # 輸入的內容與答案\n",
    "            input_content=each_data['content']\n",
    "            input_question=each_data['question']\n",
    "            truth_answer_number=each_data['answer']\n",
    "\n",
    "            result=os_ap_sss_answer(prompt,input_content,input_question,chunck_size,overlap)\n",
    "            # 對答案計分數\n",
    "            if pick_up(result)==truth_answer_number:\n",
    "                scores+=1\n",
    "            num+=1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return scores\n",
    "\n",
    "def prompt_not_in_list(prompt_list, new_prompt):\n",
    "    for item in prompt_list:\n",
    "        if item['prompt'] == new_prompt:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# 你要拿來練蠱的Training data\n",
    "training_data=data[-100:]\n",
    "# 練蠱終止條件(我是設超過baseline做100題後的分數，這邊看你資料量來設)\n",
    "stop_score=100\n",
    "# 或是設一個回合數來終止(本來我會讓他跑到天荒地老所以沒有用for loop)\n",
    "stop_run_num=20\n",
    "# 你要給LLM看幾個example\n",
    "example_num=5\n",
    "\n",
    "# 給幾個初始化prompt\n",
    "prompt_list=[\"\",\"Let's think step by step.\",\"Identify the specific information that responds to the question in the article.\",\"\"\"The text content should be concise and follow a universal structure.\n",
    "    To ensure a correct answer, the process of creating a summary should focus on identifying the main points and key details of the text.\n",
    "    It should avoid including specific content or names from the original article and instead provide a general overview of the information.\n",
    "    The summary should follow a universal structure, presenting the main idea and supporting details in a clear and concise manner.\n",
    "    By following these guidelines, the summary can accurately reflect the content of the text and lead to a correct answer.\"\"\"]\n",
    "prompt_scores=[]\n",
    "# 初始化prompt算分，這邊先用最後10筆示範\n",
    "for prompt in prompt_list:\n",
    "    score=get_score(3000,10,prompt,training_data)\n",
    "    prompt_scores.append({'prompt': prompt, 'score': score})\n",
    "\n",
    "sorted_prompt_scores = sorted(prompt_scores, key=lambda x: x['score'], reverse=True)\n",
    "print(f\"initial prompt:\\n{sorted_prompt_scores}\")\n",
    "# 開練\n",
    "while sorted_prompt_scores[0]['score']<stop_score:\n",
    "    stop_run_num-=1\n",
    "    if stop_run_num<0:\n",
    "        break\n",
    "    # 單純用來終止迴圈 你也可以interrupt\n",
    "    if os.path.exists(os.path.join(os.getcwd(),'stop_true.txt')):\n",
    "        break\n",
    "\n",
    "    # 排序你的prompt 拿比較高分的給LLM當example\n",
    "    sorted_prompt_scores = sorted(prompt_scores, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    # 整理example格式\n",
    "    example=\"\"\n",
    "    for p in sorted_prompt_scores[:example_num]:\n",
    "        example+=f\"\"\"[Old prompt]:\"{p['prompt']}\"\n",
    "        [Scores]:{p['score']}\"\"\"\n",
    "\n",
    "    # 整理輸入(可以print出來檢查)\n",
    "    input_to_temperature_llm=f\"\"\"You are an expert at crafting prompts.\n",
    "    Based on the example task given below for an LLM, fill in the most suitable prompt in the place marked [new_prompt].\n",
    "    The following describes the task you will undertake:\n",
    "\n",
    "    \"\n",
    "    Article excerpt:\n",
    "    [article_chunk]\n",
    "\n",
    "    The above is the article excerpt related to my question.\n",
    "    Below is the question I want to ask.\n",
    "    Please select the text content that can answer this question.\n",
    "    [new_prompt]\n",
    "\n",
    "    Question:\n",
    "    [input_question]\n",
    "    \"\n",
    "\n",
    "    Here are some example prompts and their scores, ranging from 0 to 100, with higher scores indicating better performance.\n",
    "    Please help me think of a unique new_prompt where higher scores are better.\n",
    "\n",
    "    {example}\n",
    "\n",
    "    ### You only need to return the new_prompt ###\n",
    "    DON'T return the [Scores] or explanation.\n",
    "    Your new_prompt:__\"\"\"\n",
    "\n",
    "    new_prompt=get_breeze_t(input_to_temperature_llm,1)\n",
    "    print(\"=\"*50)\n",
    "    print(new_prompt)\n",
    "    if prompt_not_in_list(prompt_scores,new_prompt):\n",
    "        score=get_score(3000,10,prompt,training_data)\n",
    "        print(\"*\"*50)\n",
    "        print(f\"{new_prompt}\\n{score}\")\n",
    "        prompt_scores.append({'prompt': new_prompt, 'score': score})\n",
    "    else:\n",
    "        print(\"prompt exict\")\n",
    "summary_prompt=sorted_prompt_scores[0]['prompt']\n",
    "print(f\"\"\"新的Prompt: \n",
    "      {summary_prompt}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'The text content should be concise and follow a universal structure.\\n    To ensure a correct answer, the process of creating a summary should focus on identifying the main points and key details of the text.\\n    It should avoid including specific content or names from the original article and instead provide a general overview of the information.\\n    The summary should follow a universal structure, presenting the main idea and supporting details in a clear and concise manner.\\n    By following these guidelines, the summary can accurately reflect the content of the text and lead to a correct answer.',\n",
       "  'score': 7},\n",
       " {'prompt': ' \"The text content should be concise and follow a universal structure. To ensure a correct answer, the process of creating a summary should focus on identifying the main points and key details of the text. It should avoid including specific content or names from the original article and instead provide a general overview of the information. The summary should follow a universal structure, presenting the main idea and supporting details in a clear and concise manner. By following these guidelines, the summary can accurately reflect the content of the text and lead to a correct answer.\"',\n",
       "  'score': 7},\n",
       " {'prompt': ' The new_prompt: \"Create a concise and clear summary of the article excerpt, focusing on the main points and key details that directly answer the question.\"',\n",
       "  'score': 7},\n",
       " {'prompt': ' Create a concise and clear summary of the article excerpt, focusing on the main points and key details that directly answer the question.',\n",
       "  'score': 7},\n",
       " {'prompt': '', 'score': 6},\n",
       " {'prompt': \"Let's think step by step.\", 'score': 6},\n",
       " {'prompt': 'Identify the specific information that responds to the question in the article.',\n",
       "  'score': 6}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_prompt_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用新prompt實際跑實驗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(input_data,summary_prompt,chunck_size,overlap):\n",
    "    # 輸出結果\n",
    "    my_result=[]\n",
    "    # 錯誤資料\n",
    "    error_list=[]\n",
    "    # 總分數\n",
    "    scores=0 \n",
    "    # 資料計數器\n",
    "    num=0 \n",
    "\n",
    "    for each_data in tqdm(input_data):\n",
    "        try:\n",
    "            # 輸入的內容與答案\n",
    "            input_content=each_data['content']\n",
    "            input_question=each_data['question']\n",
    "            truth_answer_number=each_data['answer']\n",
    "\n",
    "            result=os_ap_sss_answer(summary_prompt,input_content,input_question,chunck_size,overlap)\n",
    "            # 對答案計分數\n",
    "            if pick_up(result)==truth_answer_number:\n",
    "                scores+=1\n",
    "            num+=1\n",
    "\n",
    "            # print出目前得分正確率\n",
    "            print(f\"score:{scores}/{num} :({scores/num*100}%)\")\n",
    "            my_result.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_list.append(each_data)\n",
    "            print(e)\n",
    "            print(f\"目前錯誤數量:{len(error_list)}\")\n",
    "    return f\"score:{scores}/{num} :({scores/num*100}%)\",my_result,error_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score,results,error_results=run(data[:100],summary_prompt,3000,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
