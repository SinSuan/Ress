{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 實驗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "CONFIG = configparser.ConfigParser()\n",
    "PATH_CONFIG = os.getenv('path_2_config')\n",
    "\n",
    "ENVIRONMENT = os.getenv('environment')\n",
    "if ENVIRONMENT==\"windows\":\n",
    "    CONFIG.read(PATH_CONFIG, encoding='utf-8')\n",
    "else:\n",
    "    CONFIG.read(PATH_CONFIG)\n",
    "\n",
    "DEBUGGER = CONFIG[\"DEBUGGER\"][\"DEBUGGER\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超參數設定\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 停止條件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_STOP_FILE = os.path.join(os.getcwd(), 'stop_true.txt') # 人工 early stop 的檔案位置\n",
    "NUM_NEW_PROMPT = 20   # 必須是 prompt_population 的倍數\n",
    "STOP_SCORE=100  # 練蠱終止條件(我是設超過baseline做100題後的分數，這邊看你資料量來設)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "folder_record = Path(CONFIG[\"datapath\"][\"record_folder\"])\n",
    "name_experiment = \"experiment_test\" # 主要是改這行\n",
    "FOLDER_EXPERIMENT = folder_record / name_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 製作初始化資料\n",
    "\n",
    "**只有要建立新資料的時候才需要跑**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vicky\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前使用模型為:Breeze\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [06:43<00:00, 13.44s/it]\n",
      "100%|██████████| 7/7 [01:37<00:00, 13.89s/it]\n",
      "100%|██████████| 30/30 [07:27<00:00, 14.90s/it]\n",
      "100%|██████████| 7/7 [01:40<00:00, 14.35s/it]\n",
      "100%|██████████| 30/30 [06:49<00:00, 13.65s/it]\n",
      "100%|██████████| 7/7 [01:36<00:00, 13.77s/it]\n",
      "100%|██████████| 30/30 [07:32<00:00, 15.07s/it]\n",
      "100%|██████████| 7/7 [01:42<00:00, 14.64s/it]\n"
     ]
    }
   ],
   "source": [
    "from main import init_setting\n",
    "\n",
    "type_llm = \"Breeze\"\n",
    "# type_embedding = \"multi-qa-mpnet-base-dot-v1\"\n",
    "type_embedding = \"bgem3\"\n",
    "# type_embedding = None\n",
    "\n",
    "# path_prompt = CONFIG[\"datapath\"][\"init_os_prompt\"]\n",
    "# path_prompt = \"D:\\\\實習\\\\evoprompt\\\\Ress\\\\dataset\\\\init_os_prompt_corpus_2024_0724_1357.json\"\n",
    "path_prompt = \"D:\\\\實習\\\\evoprompt\\\\Ress\\\\dataset\\\\init_os_prompt_corpus.json\"\n",
    "\n",
    "path_data = CONFIG[\"datapath\"][\"Final_Quality\"]\n",
    "print(f\"{path_data=}\")\n",
    "\n",
    "ttl_model, ttl_dataset, ttl_pair_os_prompt_scores = init_setting(type_llm, type_embedding, path_data, path_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils.tools import get_file_name, time_now\n",
    "\n",
    "t = time_now()\n",
    "path_folder = \"D:\\\\實習\\\\evoprompt\\\\Ress\\\\init_prompt\"\n",
    "path_file = f\"{path_folder}/{t}_20data.json\"\n",
    "data = {\n",
    "    \"corpus\": get_file_name(path_data),\n",
    "    \"type_llm\": type_llm,\n",
    "    \"type_embedding\": type_embedding,\n",
    "    \"prompt_popularion\": ttl_pair_os_prompt_scores,\n",
    "    \"data\": ttl_dataset\n",
    "}\n",
    "with open(path_file, 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from utils.call_model.embedding import Encoder\n",
    "from utils.call_model.llm import LLM\n",
    "\n",
    "def get_init(path_init = None):\n",
    "    \n",
    "    if path_init==None:\n",
    "        path_init = \"D:\\\\實習\\\\evoprompt\\\\Ress\\\\init_prompt\\\\2024_0726_1212_data_30-7.json\"\n",
    "        # path_init = \"D:\\\\實習\\\\evoprompt\\\\Ress\\\\init_prompt\\\\test.json\"\n",
    "    with open(path_init,  'r') as file:\n",
    "        record_init = json.load(file)\n",
    "\n",
    "    # 指定 llm\n",
    "    llm = LLM(record_init[\"type_llm\"])\n",
    "\n",
    "    # 指定 embedding model\n",
    "    if record_init[\"type_embedding\"] is None:\n",
    "        embedding_model = None\n",
    "    else:\n",
    "        embedding_model = Encoder(record_init[\"type_embedding\"])\n",
    "\n",
    "    ttl_model = (llm, embedding_model)\n",
    "    \n",
    "    return ttl_model, record_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vicky\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from utils.ReSS import ReSS\n",
    "from utils.tools import time_now\n",
    "\n",
    "def test_ReSS():\n",
    "    \"\"\" 每生成一個 prompt 就存一次 population\n",
    "    \"\"\"\n",
    "    ttl_model, record_init = get_init()\n",
    "    corpus = record_init[\"corpus\"]\n",
    "    type_llm = record_init[\"type_llm\"]\n",
    "    type_embedding = record_init[\"type_embedding\"]\n",
    "    ttl_pair_os_prompt_scores = record_init[\"prompt_popularion\"]\n",
    "    ttl_dataset = record_init[\"data\"]\n",
    "    \n",
    "    ttl_dataset = {\n",
    "        \"train_split\": ttl_dataset[\"train_split\"][:1],\n",
    "        \"dev_split\": ttl_dataset[\"dev_split\"][:1]\n",
    "    }\n",
    "\n",
    "    # stop_run_num = NUM_NEW_PROMPT   # 或是設一個回合數來終止(本來我會讓他跑到天荒地老所以沒有用for loop)\n",
    "    stop_run_num = 1\n",
    "\n",
    "    record_population = []\n",
    "\n",
    "    # record_population.append(ttl_pair_os_prompt_scores)\n",
    "    sorted_pair = sorted(ttl_pair_os_prompt_scores,  key=lambda x: x['train_score'],  reverse=True)\n",
    "    while(\n",
    "        sorted_pair[0]['train_score']<STOP_SCORE\n",
    "        and stop_run_num>0\n",
    "        and not os.path.exists(PATH_STOP_FILE)   # 人工 early stop\n",
    "    ):\n",
    "        new_population = ReSS(ttl_model, ttl_dataset, sorted_pair)\n",
    "        \n",
    "        record_population.append(new_population)\n",
    "        sorted_pair = sorted(new_population,  key=lambda x: x['train_score'],  reverse=True)\n",
    "\n",
    "        stop_run_num -= 1\n",
    "\n",
    "    # 儲存結果\n",
    "    t = time_now()\n",
    "    file_path = FOLDER_EXPERIMENT / f\"{t}_ReSS.json\"\n",
    "    data = {\n",
    "        \"corpus\": corpus,\n",
    "        \"type_llm\": type_llm,\n",
    "        \"type_embedding\": type_embedding,\n",
    "        \"best_promt\": sorted_pair[0],\n",
    "        \"record\": record_population\n",
    "    }\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    print(f\"\\n\\n\\nthe result is saved at:\\n{file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\n",
      "100%|██████████| 1/1 [00:18<00:00, 18.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "the result is saved at:\n",
      "D:\\實習\\evoprompt\\Ress\\record\\experiment_test\\2024_0726_1653_ReSS.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    test_ReSS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EvoDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vicky\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from utils.EvoPrompt import EvoDE\n",
    "from utils.tools import time_now\n",
    "\n",
    "def test_EvoDE():\n",
    "    \"\"\" 每更新一次整個 population 才儲存一次\n",
    "    \"\"\"\n",
    "    t = time_now()\n",
    "    path_folder = FOLDER_EXPERIMENT / f\"{t}_Evo\"\n",
    "    path_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    ttl_model, record_init = get_init()\n",
    "    corpus = record_init[\"corpus\"]\n",
    "    type_llm = record_init[\"type_llm\"]\n",
    "    type_embedding = record_init[\"type_embedding\"]\n",
    "    ttl_pair_os_prompt_scores = record_init[\"prompt_popularion\"]\n",
    "    ttl_dataset = record_init[\"data\"]\n",
    "    \n",
    "    # num_test = 1\n",
    "    # ttl_dataset = {\n",
    "    #     \"train_split\": ttl_dataset[\"train_split\"][:num_test],\n",
    "    #     \"dev_split\": ttl_dataset[\"dev_split\"][:num_test]\n",
    "    # }\n",
    "\n",
    "    stop_run_num = int(NUM_NEW_PROMPT/4)   # 或是設一個回合數來終止(本來我會讓他跑到天荒地老所以沒有用for loop)\n",
    "    # stop_run_num=1\n",
    "\n",
    "    record_population = []\n",
    "\n",
    "    record_population.append(ttl_pair_os_prompt_scores)\n",
    "    sorted_pair = sorted(ttl_pair_os_prompt_scores,  key=lambda x: x['train_score'],  reverse=True)\n",
    "    while(\n",
    "        sorted_pair[0]['train_score']<STOP_SCORE\n",
    "        and stop_run_num>0\n",
    "        and not os.path.exists(PATH_STOP_FILE)   # 人工 early stop\n",
    "    ):\n",
    "        new_population = EvoDE(ttl_model, ttl_dataset, sorted_pair)\n",
    "        \n",
    "        record_population.append(new_population)\n",
    "        sorted_pair = sorted(new_population,  key=lambda x: x['train_score'],  reverse=True)\n",
    "\n",
    "        # 儲存結果\n",
    "        file_path = path_folder / f\"{int(NUM_NEW_PROMPT/4) - stop_run_num:02d}_EvoDE.json\"\n",
    "        data = {\n",
    "            \"corpus\": corpus,\n",
    "            \"type_llm\": type_llm,\n",
    "            \"type_embedding\": type_embedding,\n",
    "            \"best_promt\": sorted_pair[0],\n",
    "            \"record\": sorted_pair\n",
    "        }\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "        print(f\"\\n\\n\\nthe result is saved at:\\n{file_path}\")\n",
    "        \n",
    "        stop_run_num -= 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response=<Response [200]>\n",
      "j_result={'generated_text': \" Identifying the different parts between Prompt 1 and Prompt 2:\\nPrompt 1: The text content should be concise and follow a universal structure.\\nTo ensure a correct answer, the process of creating a summary should focus on identifying the main points and key details of the text.\\nIt should avoid including specific content or names from the original article and instead provide a general overview of the information.\\nThe summary should follow a universal structure, presenting the main idea and supporting details in a clear and concise manner.\\nBy following these guidelines, the summary can accurately reflect the content of the text and lead to a correct answer.\\nPrompt 2: The text content should be concise and follow a universal structure.\\nTo ensure a correct answer, the process of creating a summary should focus on identifying the main points and key details of the text.\\nIt should avoid including specific content or names from the original article and instead provide a general overview of the information.\\nThe summary should follow a universal structure, presenting the main idea and supporting details in a clear and concise manner.\\nBy following these guidelines, the summary can accurately reflect the content of the text and lead to a correct answer.\\n2. Randomly mutate the different parts\\n3. Combine the different parts with Prompt 3, selectively replace it with the different parts in step 2 and generate a new prompt.\\nPrompt 3:\\n4. Crossover the prompt in step 3 with the following basic prompt and generate a final prompt bracketed with <prompt> and </prompt>:\\nBasic Prompt: Let's think step by step.\\nFinal Prompt: <prompt>Create a concise summary that follows a universal structure, focuses on main points and key details, and avoids specific content or names from the original article, providing a general overview of the information.</prompt>\"}\n",
      "new_prompt='Create a concise summary that follows a universal structure, focuses on main points and key details, and avoids specific content or names from the original article, providing a general overview of the information.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:02<01:08,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response=<Response [200]>\n",
      "j_result={'generated_text': ' Answer__option3'}\n",
      "response=<Response [200]>\n",
      "j_result={'generated_text': ' Open access (OA) is a revolutionary approach to sharing research literature that removes price and permission barriers, making it free and accessible to readers worldwide. Conventional publishing, on the other hand, involves charging for access to research articles and often imposes copyright and licensing restrictions. The relationship between conventional publishing and open access is that they serve different purposes and cater to the needs of authors and readers in different ways. While conventional publishing may be better for authors who rely on sales for income, open access benefits readers by providing free and unrestricted access to research. As the access revolution continues, both conventional publishing and open access are adapting to the needs of authors and readers, with many people interacting with both and venues picking one or the other option based on their specific goals and purposes.'}\n"
     ]
    }
   ],
   "source": [
    "pre_file_path = None\n",
    "for i in range(5):\n",
    "    test_EvoDE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EvoGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from utils.EvoPrompt import EvoGA\n",
    "from utils.tools import time_now\n",
    "\n",
    "def test_EvoGA():\n",
    "    \"\"\" 每更新一次整個 population 才儲存一次\n",
    "    \"\"\"\n",
    "    t = time_now()\n",
    "    path_folder = FOLDER_EXPERIMENT / f\"{t}_EvoGA\"\n",
    "    path_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    ttl_model, record_init = get_init()\n",
    "    corpus = record_init[\"corpus\"]\n",
    "    type_llm = record_init[\"type_llm\"]\n",
    "    type_embedding = record_init[\"type_embedding\"]\n",
    "    ttl_pair_os_prompt_scores = record_init[\"prompt_popularion\"]\n",
    "    ttl_dataset = record_init[\"data\"]\n",
    "    \n",
    "    ttl_dataset = {\n",
    "        \"train_split\": ttl_dataset[\"train_split\"][:1],\n",
    "        \"dev_split\": ttl_dataset[\"dev_split\"][:1]\n",
    "    }\n",
    "\n",
    "    # stop_run_num = int(NUM_NEW_PROMPT/4)   # 或是設一個回合數來終止(本來我會讓他跑到天荒地老所以沒有用for loop)\n",
    "    stop_run_num=1\n",
    "\n",
    "    record_population = []\n",
    "\n",
    "    record_population.append(ttl_pair_os_prompt_scores)\n",
    "    sorted_pair = sorted(ttl_pair_os_prompt_scores,  key=lambda x: x['train_score'],  reverse=True)\n",
    "    while(\n",
    "        sorted_pair[0]['train_score']<STOP_SCORE\n",
    "        and stop_run_num>0\n",
    "        and not os.path.exists(PATH_STOP_FILE)   # 人工 early stop\n",
    "    ):\n",
    "        new_population = EvoGA(ttl_model, ttl_dataset, sorted_pair)\n",
    "        \n",
    "        record_population.append(new_population)\n",
    "        sorted_pair = sorted(new_population,  key=lambda x: x['train_score'],  reverse=True)\n",
    "\n",
    "        # 儲存結果\n",
    "        file_path = f\"{path_folder}\\\\{int(NUM_NEW_PROMPT/4) - stop_run_num+2:02d}_EvoGA.json\"\n",
    "        data = {\n",
    "            \"corpus\": corpus,\n",
    "            \"type_llm\": type_llm,\n",
    "            \"type_embedding\": type_embedding,\n",
    "            \"best_promt\": sorted_pair[0],\n",
    "            \"record\": sorted_pair\n",
    "        }\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "        print(f\"\\n\\n\\nthe result is saved at:\\n{file_path}\")\n",
    "        \n",
    "        stop_run_num -= 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n",
      "100%|██████████| 1/1 [00:27<00:00, 27.48s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "100%|██████████| 1/1 [00:26<00:00, 26.82s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "100%|██████████| 1/1 [00:25<00:00, 25.16s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "100%|██████████| 1/1 [00:23<00:00, 23.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "the result is saved at:\n",
      "D:\\實習\\evoprompt\\Ress\\record\\experiment_test\\2024_0726_1715_EvoGA\\06_EvoGA.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pre_file_path = None\n",
    "for i in range(1):\n",
    "    test_EvoGA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
